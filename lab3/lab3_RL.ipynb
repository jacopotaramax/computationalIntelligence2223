{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.4 Reinforced Learning\n",
    "\n",
    "\n",
    "## Task\n",
    "\n",
    "Write agents able to play [*Nim*](https://en.wikipedia.org/wiki/Nim), with an arbitrary number of rows and an upper bound $k$ on the number of objects that can be removed in a turn (a.k.a., *subtraction game*).\n",
    "\n",
    "The player **taking the last object wins**.\n",
    "\n",
    "\n",
    "An agent using reinforcement learning\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util functions from previous task\n",
    "Nim class has been changed a little: we add avaible_actions that are all tha actions avaible for each state, it has been taken by the old function 'cooked' in the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from typing import Callable\n",
    "from itertools import product, accumulate\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "#Variables\n",
    "NUM_MATCHES = 1000\n",
    "NUM_TRAINING_TRIALS = 10000\n",
    "NIM_SIZE = 5\n",
    "\n",
    "\n",
    "Nimply = namedtuple(\"Nimply\", \"row, num_objects\")\n",
    "\n",
    "class Nim():\n",
    "    def __init__(self, num_rows: int, player=0):\n",
    "        self._rows = [i * 2 + 1 for i in range(num_rows)]\n",
    "        self._player = player\n",
    "        \n",
    "    def nimming(self, ply: Nimply) -> None:\n",
    "        row, num_objects = ply\n",
    "        assert self._rows[row] >= num_objects\n",
    "        self._rows[row] -= num_objects\n",
    "    \n",
    "    def __bool__(self):\n",
    "        return sum(self._rows) > 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<\" + \" \".join(str(_) for _ in self._rows) + \">\"\n",
    "\n",
    "    @property\n",
    "    def rows(self) -> tuple:\n",
    "        return tuple(self._rows)\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        return self._k\n",
    "       \n",
    "        \n",
    "    def available_actions(rows):\n",
    "        '''\n",
    "        computes all the avaible actions as tuples (rowline,numobjectstoremove)\n",
    "        '''\n",
    "        actions = set()\n",
    "        for r, c in enumerate(rows):\n",
    "            for o in range(1, c + 1):\n",
    "                actions.add((r, o))\n",
    "        return actions\n",
    "\n",
    "\n",
    "def cookStatus(state: Nim) -> dict:\n",
    "    cooked = dict()\n",
    "    cooked[\"possibleMoves\"] = [\n",
    "        (r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) \n",
    "    ]\n",
    "    cooked[\"activeRowsNumber\"] = sum(o > 0 for o in state.rows)\n",
    "    cooked[\"totalElements\"] = sum(state.rows)\n",
    "    cooked[\"shortestRow\"] = min((x for x in enumerate(state.rows) if x[1] > 0), key=lambda y: y[1])[0]\n",
    "    cooked[\"longestRow\"] = max((x for x in enumerate(state.rows)), key=lambda y: y[1])[0]\n",
    "\n",
    "    bruteForce = list()\n",
    "    for m in cooked[\"possibleMoves\"]:\n",
    "        tmp = deepcopy(state)\n",
    "        tmp.nimming(m)\n",
    "        bruteForce.append((m, tuple(tmp._rows)))\n",
    "    cooked[\"bruteForce\"] = bruteForce\n",
    "\n",
    "    return cooked\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent using Reinforcement Learning (Q-Learning)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Q-learning\n",
    "\n",
    "The agent is using a model-free method: it will learn exclusively from trial and error (no modelling of the environment). \n",
    "Reinforced learning involves an agent, a set of states S and a set of actions A per state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_RL():\n",
    "    '''\n",
    "    Class based on Q-Learning: For any finite Markov decision process (FMDP), \n",
    "    Q-learning finds an optimal policy in the sense of maximizing the expected \n",
    "    value of the total reward over any and all successive steps, \n",
    "    starting from the current state.\n",
    "    Q-learning can identify an optimal action-selection policy for any given FMDP\n",
    "    from wikipedia\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, alpha = 0.15, random_factor = 0.2): # 80% explore, 20% exploit \n",
    "        ''' We start from an empty Q dictionary that will contain all the tuples \n",
    "        state/action mapped as Q-values\n",
    "        '''\n",
    "        # DICTIONARY Q:  \n",
    "        self.q = dict()\n",
    "\n",
    "        # ALPHA: The learning rate or step size determines \n",
    "        # to what extent newly acquired information overrides old information\n",
    "        self.alpha = alpha \n",
    "        self.random_factor = random_factor\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        '''\n",
    "        We get the value of Q taking as inputs (state and action), if there is not yet we give q=0\n",
    "        '''\n",
    "        state = tuple(state)\n",
    "\n",
    "        q_value = 0 # since Q-Learning is a iterative algorithm we must initializate the q_value to 0 \n",
    "        if (state, action) in self.q: # finds the q_value in the dict of q values\n",
    "            q_value = self.q[state, action]\n",
    "        return q_value\n",
    "\n",
    "    def best_future_reward(self, state):\n",
    "        '''\n",
    "        As input we take the Board state and then we compute all the possible actions\n",
    "        '''\n",
    "        actions = Nim.available_actions(state)\n",
    "\n",
    "        # Check if there are no action possible, else go on\n",
    "        if len(actions) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            scores = list()\n",
    "            for action in actions:\n",
    "                score = self.get_q_value(state, action) #we can take the Q dictionary values \n",
    "                scores.append(score) \n",
    "        \n",
    "        # The goal of the agent is to maximize its total reward. It does this by adding the maximum reward\n",
    "        # attainable from future states to the reward for achieving its current state\n",
    "        return max(scores)\n",
    "\n",
    "    def choose_action(self, state, random_factor=True):\n",
    "        '''\n",
    "        returns an action to take once receive a state as input\n",
    "        '''\n",
    "        actions = Nim.available_actions(state)\n",
    "\n",
    "        # If random_factor mode is enabled with probability random_factor, choose a random item\n",
    "        if random.random() < self.random_factor:\n",
    "            return random.choice(list(actions))\n",
    "        else:\n",
    "            actions_by_score = dict()\n",
    "            for action in actions:\n",
    "                score = self.get_q_value(state, action)\n",
    "                actions_by_score[score] = action\n",
    "            scores = list(actions_by_score.keys())  # Find the largest key in dictionary\n",
    "            best_action = actions_by_score[max(scores)]             \n",
    "            return best_action # We return the best action (q max) if the random state is disabled\n",
    "        \n",
    "\n",
    "    def update_q_value(self, state, action, old_q, reward, future_rewards):\n",
    "        '''\n",
    "        update w value once the action has been taken, there's also an estimate of the future reward\n",
    "\n",
    "        '''\n",
    "        state = tuple(state)\n",
    "        # Q (state,action) = previous Q value + alpha_learning_rate * (new value estimate - olq Q value)\n",
    "        # with new value estimate that is the current reward + the future reward estimation\n",
    "        q_value = old_q + self.alpha*(reward + future_rewards)\n",
    "        self.q[(state, action)] = q_value\n",
    "        return\n",
    "\n",
    "    def update(self, old_state, action, new_state, reward):\n",
    "        \n",
    "        '''\n",
    "        Q-learning mode updates when performing the action on the old board state.\n",
    "        '''\n",
    "\n",
    "        old_q_value = self.get_q_value(old_state, action)\n",
    "        best_future = self.best_future_reward(new_state)\n",
    "        self.update_q_value(old_state, action, old_q_value, reward, best_future)\n",
    "\n",
    "def Learn(num_games: int):\n",
    "    '''Strategy Q-learning by playing free methods vs itself'''\n",
    "    \n",
    "    player = Agent_RL()\n",
    "    \n",
    "    for game in range(num_games):\n",
    "        #print(f\"Playing training game {game + 1}\")\n",
    "        game = Nim(NIM_SIZE)\n",
    "\n",
    "        # History of the actions taken by the players, erased for each game\n",
    "        last = {\n",
    "            0: {\"state\": None, \"action\": None},\n",
    "            1: {\"state\": None, \"action\": None}\n",
    "        }\n",
    "\n",
    "        # Play\n",
    "        while True:\n",
    "            state = game._rows.copy()\n",
    "            action = player.choose_action(game._rows)\n",
    "            \n",
    "            # Keep track of last state and action\n",
    "            last[game._player][\"state\"] = state\n",
    "            last[game._player][\"action\"] = action\n",
    "          \n",
    "            # Take actions\n",
    "            game.nimming(action)\n",
    "            new_state = game._rows.copy()\n",
    "            \n",
    "            # When we got a winner we can update rewards\n",
    "            if game:\n",
    "                player.update(state, action, new_state, -1)\n",
    "                player.update(last[game._player][\"state\"],last[game._player][\"action\"],new_state,1)\n",
    "                break\n",
    "            elif last[game._player][\"state\"] is not None:\n",
    "                player.update(last[game._player][\"state\"],last[game._player][\"action\"],new_state,0)\n",
    "            \n",
    "            break\n",
    "            \n",
    "    print(f'Training done with {NUM_TRAINING_TRIALS} games')\n",
    "    \n",
    "    return player\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_random(state: Nim) -> Nimply:\n",
    "    '''simple strategy, it just choose randomly from the rows'''\n",
    "    row = random.choice([r for r, c in enumerate(state.rows) if c > 0])\n",
    "    num_objects = random.randint(1, state.rows[row])\n",
    "    return Nimply(row, num_objects)\n",
    "\n",
    "def renforced(state: Nim, R: Agent_RL) -> Nimply:\n",
    "    row, num_objects = R.choose_action(state._rows, random_factor=False)\n",
    "    #row = random.choice([r for r, c in enumerate(state.rows) if c > 0])\n",
    "    #num_objects = random.randint(1, state.rows[row])\n",
    "    return Nimply(row, num_objects)\n",
    "\n",
    "def evaluate_LR(strategy1: Callable, strategy2: Callable, R: Agent_RL) -> float:\n",
    "    '''evaluate put a first strategy versus a second one for a certain number of times, the results is the ratio \n",
    "    between the won matches and the total number of played matches'''\n",
    "    won = 0\n",
    "\n",
    "    for m in range(NUM_MATCHES):\n",
    "        nim = Nim(NIM_SIZE)\n",
    "        #player = 1\n",
    "        player = random.randint(0,1) #make a random start\n",
    "        while nim:\n",
    "            if player == 0:\n",
    "                ply = strategy1(nim, R)\n",
    "            else:\n",
    "                ply = strategy2(nim)\n",
    "            #ply = opponent[player](nim)\n",
    "            nim.nimming(ply)\n",
    "            player = 1 - player\n",
    "        if player == 1:\n",
    "            won += 1\n",
    "    return round(won / NUM_MATCHES, 2)\n",
    "\n",
    "def gabriele(state: Nim) -> Nimply:\n",
    "    \"\"\"Pick always the maximum possible number of the lowest row\"\"\"\n",
    "    possible_moves = [(r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1)]\n",
    "    return Nimply(*max(possible_moves, key=lambda m: (-m[0], m[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done with 10000 games\n",
      "0.52\n"
     ]
    }
   ],
   "source": [
    "PlayerLR = Learn(NUM_TRAINING_TRIALS)\n",
    "ev_RLvsRandom = evaluate_LR(renforced, pure_random, PlayerLR)\n",
    "logging.info(f\"Expert vs optimal => {ev_RLvsRandom}\")\n",
    "print(ev_RLvsRandom)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy from MAZE Project\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_MZ(object):\n",
    "        def __init__(self, state, alpha=0.15, random_factor=0.2):\n",
    "            self.state_history = [(tuple(state._rows), 0)]  # 80% explore, 20% exploit\n",
    "            self.alpha = alpha\n",
    "            self.random_factor = random_factor\n",
    "            self.G = {}\n",
    "            self.init_reward(state)\n",
    "\n",
    "        def init_reward(self, state):\n",
    "            rows = list()\n",
    "            # we do not have state as in maze so we have to use the _rows in nim\n",
    "            state = state._rows\n",
    "            for i in state:\n",
    "                rows.append(list(range(i+1)))\n",
    "            \n",
    "            for row in product(*rows):\n",
    "                self.G[row] = np.random.uniform(low=1.0, high=0.1)\n",
    "\n",
    "        def choose_action(self, state, allowedpossibleMoves):\n",
    "            #allowed possibleMoves arrives from cook functions as a list\n",
    "            maxG = -10e15\n",
    "            next_move = None\n",
    "            randomN = np.random.random()\n",
    "            if randomN < self.random_factor:\n",
    "                # if random number below random factor, choose random action\n",
    "                random_idx = np.random.choice(len(allowedpossibleMoves))\n",
    "                return allowedpossibleMoves[random_idx]\n",
    "            else:\n",
    "                # if exploiting, gather all possible actions and choose one with the highest G (reward)\n",
    "                for action in allowedpossibleMoves:\n",
    "                    new_state = deepcopy(state)\n",
    "                    new_state.nimming(action)\n",
    "                    if self.G[tuple(new_state._rows)] >= maxG:\n",
    "                        maxG = self.G[tuple(new_state._rows)]\n",
    "                        return action\n",
    "\n",
    "            return next_move\n",
    "\n",
    "        def update_state_history(self, state, reward):\n",
    "            self.state_history.append((tuple(state._rows), reward))\n",
    "\n",
    "        def learn(self):\n",
    "            target = 0\n",
    "\n",
    "            for prev, reward in reversed(self.state_history):\n",
    "                self.G[prev] = self.G[prev] + self.alpha * (target - self.G[prev])\n",
    "                target += reward\n",
    "\n",
    "            self.state_history = []\n",
    "\n",
    "            self.random_factor -= 10e-5  # decrease random factor each episode of play\n",
    "\n",
    "def RLAgent(G: dict) -> Nimply:\n",
    "    def evolvable(state: Nim) -> Nimply:\n",
    "        #possibleStates = [(r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1)]\n",
    "        possibleStates = cookStatus(state)[\"bruteForce\"]\n",
    "        ply = max(\n",
    "            ((state[0], G[state[1]]) for state in possibleStates if state[1] in G), \n",
    "            key=lambda i: i[1]\n",
    "            )[0]\n",
    "        return Nimply(ply[0], ply[1])\n",
    "    return evolvable\n",
    "\n",
    "def get_rewards(state):\n",
    "    sum_row = sum(i > 0 for i in state._rows)\n",
    "    if sum_row == 1:\n",
    "        return -1 # losing situation\n",
    "    elif sum_row >1:\n",
    "        return -0.5 \n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function for the Reinforced Learnign from the Maze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_MZ(strategy1: Callable, strategy2: Callable) -> float:\n",
    "    '''evaluate put a first strategy versus a second one for a certain number of times, the results is the ratio \n",
    "    between the won matches and the total number of played matches'''\n",
    "    opponent = (strategy1, strategy2)\n",
    "    won = 0\n",
    "\n",
    "    for m in range(NUM_MATCHES):\n",
    "        nim = Nim(NIM_SIZE)\n",
    "        #player = 1\n",
    "        player = random.randint(0,1) #make a random start\n",
    "        while nim:\n",
    "            ply = opponent[player](nim)\n",
    "            nim.nimming(ply)\n",
    "            player = 1 - player\n",
    "        if player == 1:\n",
    "            won += 1\n",
    "    return round(won / NUM_MATCHES, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n"
     ]
    }
   ],
   "source": [
    "state = Nim(5)\n",
    "agentRL = Agent_MZ(state, 0.2, 0.15)\n",
    "\n",
    "\n",
    "for i in range(5000):\n",
    "        #we start with a new istance of the game\n",
    "        new_state = deepcopy(state)\n",
    "        while new_state: # boolean of nim\n",
    "            possibleMoves = cookStatus(new_state)[\"possibleMoves\"]\n",
    "            action = agentRL.choose_action(new_state, possibleMoves)\n",
    "            new_state.nimming(action)\n",
    "\n",
    "            reward = get_rewards(new_state)\n",
    "            agentRL.update_state_history(new_state, reward)\n",
    "            \n",
    "            if sum(new_state._rows) == 0:\n",
    "                # we have a winner\n",
    "                break\n",
    "            new_state.nimming(pure_random(new_state))\n",
    "        agentRL.learn()\n",
    "\n",
    "\n",
    "ev_RLvsRandom = evaluate_MZ(RLAgent(agentRL.G), pure_random)\n",
    "logging.info(f\"Expert vs optimal => {ev_RLvsRandom}\")\n",
    "print(ev_RLvsRandom)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4 (tags/v3.9.4:1f2e308, Apr  6 2021, 13:40:21) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "783ea8a23e694503d3804af2ae9782b8de7168d07b90710e7b7826ea7ef3bbad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
